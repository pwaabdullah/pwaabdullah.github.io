[{"content":"The Evaluation of Recommendation Systems - Part 3 Context: Why This Post Matters, Who It‚Äôs For, and What You‚Äôll Learn Welcome to Part 3 of our four-part series on evaluating recommendation systems (RecSys)! In the previous installments, we laid the groundwork: Part 1 introduced foundational techniques like collaborative filtering (CF) and Matrix Factorization (MF), which excelled at capturing user-item interactions but assumed linearity, missing complex patterns. Part 2 explored Factorization Machines (FM) and XGBoost, which tackled sparse data and non-linear ranking but fell short on higher-order interactions and sequential behaviors. By 2016, these limitations spurred a seismic shift toward deep neural networks (DNNs), which transformed RecSys by learning intricate feature interactions, automating feature engineering, and addressing diverse tasks like sequential recommendations and multi-task optimization. This post traces that evolution from 2016 to 2023, diving into Neural Collaborative Filtering (NCF), Wide \u0026amp; Deep Learning, DeepFM, Deep Interest Network (DIN), Deep Learning Recommendation Model (DLRM), and Adaptive Task-to-Task Fusion (AdaTT). It‚Äôs tailored for data scientists, ML engineers, and tech professionals‚Äîparticularly those designing large-scale RecSys in domains like e-commerce, streaming, and advertising‚Äîwho need a deep, technical understanding of these advancements.\nRecap: Where We Left Off In Part 2, we saw how FM extended MF by modeling pairwise feature interactions, making it a powerhouse for sparse settings like click-through rate (CTR) prediction. Its prediction function, ( \\hat{y}(\\mathbf{x}) = w_0 + \\sum_{i=1}^n w_i x_i + \\sum_{i=1}^n \\sum_{j=i+1}^n \\langle v_i, v_j \\rangle x_i x_j ), captured second-order relationships efficiently but couldn‚Äôt handle higher-order interactions or non-linear patterns beyond its linear assumptions. XGBoost, meanwhile, leveraged tree ensembles to rank items based on non-linear feature combinations, shining in tasks like top-N recommendations. Yet, it struggled with high-dimensional sparse data and required extensive manual feature engineering, limiting its scalability. These gaps‚Äîmissing deep non-linearities, higher-order interactions, and sequential modeling‚Äîpaved the way for DNNs, which, starting in 2016, redefined RecSys by learning complex patterns directly from raw data.\nThe Big Picture: The Deep Learning Revolution in RecSys Picture a recommendation system as a guide helping you navigate a vast library. In Part 2, our guide used simple rules: FM paired clues like your reading history with book traits, while XGBoost ranked options by studying everyone‚Äôs preferences. But what if your interests shift over time (say, from mysteries to sci-fi), or the guide needs to predict both what you‚Äôll read and whether you‚Äôll buy it? These earlier methods faltered. DNNs emerged as a smarter guide, capable of deciphering intricate patterns, tracking sequential behaviors, and juggling multiple goals. From 2016‚Äôs Wide \u0026amp; Deep to 2023‚Äôs AdaTT, this era saw RecSys evolve to handle complex user behaviors with unprecedented accuracy, shaping modern systems in companies like Google, Alibaba, and Facebook.\nDeep Dive: The Evolution of DNNs in RecSys Let‚Äôs explore this journey, starting with Neural Collaborative Filtering, which kicked off the DNN era by rethinking how we model user-item interactions.\nNeural Collaborative Filtering (NCF, 2017) Traditional MF, a staple from Part 1, predicts user-item interactions via a dot product: ( \\hat{r}_{ui} = p_u^T q_i ), where ( p_u ) and ( q_i ) are latent vectors for user ( u ) and item ( i ). This worked well for explicit ratings but assumed linearity, missing non-linear patterns in implicit feedback like clicks or views. In 2017, He et al. proposed Neural Collaborative Filtering (NCF) to overcome this, replacing the dot product with a neural network to capture complex, non-linear relationships. The motivation was clear: real-world preferences aren‚Äôt linear‚Äîliking sci-fi movies doesn‚Äôt linearly predict liking sci-fi books‚Äîand DNNs, fresh from successes in vision and NLP, offered a way to model these nuances.\nNCF‚Äôs architecture comes in three flavors. First, the inputs are simple: one-hot encoded user ID ( \\mathbf{u} ) and item ID ( \\mathbf{i} ), mapped to dense embeddings ( \\mathbf{p}_u ) and ( \\mathbf{q}_i \\in \\mathbb{R}^{32} ) via lookup tables. The Generalized Matrix Factorization (GMF) variant mimics MF but with a neural twist: it computes an element-wise product ( \\mathbf{p}_u \\odot \\mathbf{q}i ), feeds it through a linear layer with weights ( \\mathbf{w} ), and applies a sigmoid activation to output a probability: ( \\hat{y}{ui} = \\sigma(\\mathbf{w}^T (\\mathbf{p}_u \\odot \\mathbf{q}_i)) ). This retains MF‚Äôs linear interaction but learns the weighting neurally. The Multi-Layer Perceptron (MLP) variant takes a different tack, concatenating the embeddings into ( [\\mathbf{p}_u, \\mathbf{q}_i] ) and passing them through three fully connected layers (e.g., 256, 128, 64 neurons) with ReLU activations: ( \\mathbf{z}_1 = \\text{ReLU}(\\mathbf{W}_1 [\\mathbf{p}u, \\mathbf{q}i] + \\mathbf{b}1) ), followed by more layers, ending in a prediction layer. This captures non-linear interactions unavailable to MF. Finally, Neural Matrix Factorization (NeuMF) combines both, concatenating GMF‚Äôs and MLP‚Äôs penultimate outputs and applying a final linear layer: ( \\hat{y}{ui} = \\sigma(\\mathbf{w}^T [\\mathbf{z}{\\text{GMF}}, \\mathbf{z}{\\text{MLP}}]) ). This hybrid leverages both linear and non-linear modeling.\nFor implicit feedback (e.g., clicks), NCF uses binary cross-entropy as its loss: ( L = -\\sum_{(u,i) \\in D} [y_{ui} \\log(\\hat{y}{ui}) + (1-y{ui}) \\log(1-\\hat{y}{ui})] ), where ( y{ui} = 1 ) for observed interactions and 0 otherwise. Since unobserved pairs vastly outnumber observed ones, negative sampling (e.g., 4 negatives per positive) keeps training feasible. The optimizer is Adam, with a learning rate of 0.001, balancing speed and stability. On the MovieLens 1M dataset, NeuMF achieved a Hit Ratio@10 of 0.71, beating MF‚Äôs 0.67 by 6%, thanks to its ability to model non-linear patterns. Compared to MLP alone (0.69), NeuMF‚Äôs fusion of GMF‚Äôs linearity and MLP‚Äôs depth proved superior. The special change‚Äîswapping a dot product for a neural network‚Äîunlocked this flexibility, though NCF ignores auxiliary features like user demographics and can‚Äôt model sequential behaviors.\nWide \u0026amp; Deep Learning (2016) NCF‚Äôs focus on user-item pairs left out contextual features and struggled with generalization in sparse, diverse settings. Enter Wide \u0026amp; Deep Learning, proposed by Cheng et al. at Google in 2016, designed for app recommendations on Google Play. The problem was twofold: linear models like logistic regression memorized specific patterns (e.g., ‚Äúuser installed app A‚Äù) but couldn‚Äôt generalize to unseen data, while DNNs generalized well but missed rare, critical interactions. Wide \u0026amp; Deep combined a linear ‚Äúwide‚Äù model for memorization with a DNN ‚Äúdeep‚Äù model for generalization, aiming to balance both.\nThe architecture starts with inputs: sparse features (e.g., user ID, app ID) mapped to embeddings (e.g., 32 dimensions) and dense features (e.g., user age) fed raw. The wide component is a linear model: ( \\mathbf{y}{\\text{wide}} = \\mathbf{w}^T \\mathbf{x} + b ), where ( \\mathbf{x} ) includes raw features and hand-crafted cross-features (e.g., ‚Äúuser installed app A AND app B‚Äù), capturing low-order interactions. Designing these cross-features required domain expertise, a key modification over pure DNNs. The deep component is an MLP with three hidden layers (1024, 512, 256 neurons) using ReLU: ( \\mathbf{z}1 = \\text{ReLU}(\\mathbf{W}1 \\mathbf{e} + \\mathbf{b}1) ), where ( \\mathbf{e} ) concatenates embeddings and dense inputs, learning higher-order interactions. The outputs merge via a weighted sum: ( \\hat{y} = \\sigma(\\mathbf{w}{\\text{wide}}^T \\mathbf{y}{\\text{wide}} + \\mathbf{w}{\\text{deep}}^T \\mathbf{z}{\\text{deep}} + b) ), yielding a click probability.\nThe loss is logistic (binary cross-entropy), optimized differently per component: FTRL with L1 regularization for the wide part (encouraging sparsity) and AdaGrad for the deep part (adapting to dense gradients). On Google Play, Wide \u0026amp; Deep boosted app installations by 3.9% over a wide-only model and 1% over a deep-only model, proving the hybrid‚Äôs value. Unlike NCF, it leverages auxiliary features, but the manual engineering of cross-features limits scalability, and it doesn‚Äôt address sequential data or higher-order interactions beyond the wide part.\nDeepFM (2017) Wide \u0026amp; Deep‚Äôs reliance on manual feature engineering was a bottleneck, especially for large-scale systems with thousands of features. In 2017, Guo et al. introduced DeepFM, targeting CTR prediction in online advertising (e.g., Criteo dataset), by combining Factorization Machines (FM) with a DNN to automate feature interactions. FM‚Äôs strength was modeling pairwise interactions efficiently, but it missed higher-order patterns; DeepFM extended it to capture both low- and high-order interactions without human intervention.\nDeepFM‚Äôs inputs are sparse features (e.g., user ID, ad ID) mapped to embeddings ( \\mathbf{v}i \\in \\mathbb{R}^{10} ). The FM component computes: ( \\mathbf{y}{\\text{FM}} = w_0 + \\sum_{i=1}^n w_i x_i + \\sum_{i=1}^n \\sum_{j=i+1}^n \\langle \\mathbf{v}_i, \\mathbf{v}_j \\rangle x_i x_j ), capturing second-order interactions via dot products. The deep component, an MLP with three 200-neuron layers and ReLU, takes the same embeddings: ( \\mathbf{z}1 = \\text{ReLU}(\\mathbf{W}1 \\mathbf{v} + \\mathbf{b}1) ), learning higher-order interactions. Sharing embeddings between FM and DNN ensures consistency and efficiency‚Äîa key design choice. The final output combines both: ( \\hat{y} = \\sigma(\\mathbf{y}{\\text{FM}} + \\mathbf{w}{\\text{deep}}^T \\mathbf{z}{\\text{deep}}) ).\nThe loss is binary cross-entropy, optimized with Adam (learning rate 0.001). On Criteo, DeepFM hit an AUC of 0.801, edging out Wide \u0026amp; Deep (0.799) and FM (0.785), as it automated feature engineering while retaining FM‚Äôs strengths. This modification‚Äîreplacing Wide \u0026amp; Deep‚Äôs manual cross-features with FM‚Äîmade it scalable, though it still overlooks sequential user behaviors critical for dynamic settings like e-commerce.\nDeep Interest Network (DIN, 2017) DeepFM‚Äôs static modeling couldn‚Äôt capture how user interests evolve, say, during an e-commerce browsing session. Zhou et al. at Alibaba introduced the Deep Interest Network (DIN) in 2017 to address this, using an attention mechanism to weigh historical behaviors based on their relevance to a candidate item. Proposed for ad recommendations, DIN recognized that not all past interactions (e.g., clicked items) equally inform the next click, necessitating a dynamic approach.\nDIN‚Äôs inputs include a user‚Äôs behavior sequence ( S_u = {v_1, v_2, \\ldots, v_T} ) (e.g., clicked items), a candidate ad ( v_a ), and context features, all mapped to embeddings. The core innovation is the attention mechanism: for each historical item ( v_i ), it computes a weight ( \\alpha_i = f(\\mathbf{v}_i, \\mathbf{v}_a) ) using a small MLP: ( f(\\mathbf{v}_i, \\mathbf{v}_a) = \\text{ReLU}(\\mathbf{W} [\\mathbf{v}_i, \\mathbf{v}_a, \\mathbf{v}_i \\odot \\mathbf{v}_a] + \\mathbf{b}) ). This weights items by relevance, forming a user interest vector ( \\mathbf{s}u = \\sum{i=1}^T \\alpha_i \\mathbf{v}i ). This vector, the candidate embedding, and context embeddings feed into an MLP with three layers (200, 80, 2 neurons) and ReLU, ending in a sigmoid output: ( \\hat{y} = \\sigma(\\mathbf{w}^T \\mathbf{z}{\\text{deep}}) ).\nThe loss is binary cross-entropy, optimized with Adam (learning rate 0.001). On Alibaba‚Äôs dataset, DIN‚Äôs AUC of 0.82 beat DeepFM‚Äôs 0.80 by 2%, highlighting attention‚Äôs power in sequential modeling. Unlike DeepFM, DIN adapts to temporal dynamics, but it‚Äôs tailored to single-task CTR prediction, not multi-objective scenarios.\nDeep Learning Recommendation Model (DLRM, 2019) DIN‚Äôs single-tower design wasn‚Äôt built for the massive scale and diverse features of systems like Facebook‚Äôs ad platform. In 2019, Naumov et al. proposed DLRM, a multi-tower architecture for CTR prediction, explicitly modeling pairwise interactions for scalability and interpretability. The need arose from handling billions of sparse features (e.g., ad IDs) alongside dense ones (e.g., user stats), where implicit modeling slowed training.\nDLRM‚Äôs inputs split into dense features (e.g., user age) and sparse features (e.g., user ID), mapped to embeddings. The dense tower is an MLP with three layers (512, 256, 128 neurons) and ReLU, processing continuous inputs. The sparse tower computes pairwise dot products between embeddings: ( \\mathbf{z}_{ij} = \\mathbf{v}_i^T \\mathbf{v}_j ), forming an interaction vector. These outputs concatenate with the dense tower‚Äôs result, feeding a top MLP (128, 1 neurons) with ReLU and sigmoid: ( \\hat{y} = \\sigma(\\mathbf{w}^T \\mathbf{z}) ).\nThe loss is binary cross-entropy, optimized with Adam or SGD. On Criteo, DLRM‚Äôs AUC of 0.802 slightly topped DeepFM‚Äôs 0.801, with better scalability from its parallel towers‚Äîa key change over single-tower designs. However, it focuses on single-task CTR, not multi-task or sequential needs.\nAdaptive Task-to-Task Fusion (AdaTT, 2023) DLRM‚Äôs single-task focus couldn‚Äôt handle multi-objective RecSys, like predicting CTR and conversion rate (CVR) together. Multi-Task Learning (MTL) emerged to share representations across tasks, but task conflicts often hurt performance. Yang et al.‚Äôs AdaTT, introduced in 2023 at Kuaishou, tackled this with an adaptive task-to-task fusion network, dynamically balancing task interactions.\nAdaTT‚Äôs inputs‚Äîshared features (e.g., user ID, item ID)‚Äîmap to embeddings feeding a shared bottom MLP: ( \\mathbf{z}_{\\text{shared}} = \\text{ReLU}(\\mathbf{W} \\mathbf{e} + \\mathbf{b}) ). Task-specific towers (e.g., CTR, CVR) process this into ( \\mathbf{z}t = \\text{MLP}t(\\mathbf{z}{\\text{shared}}) ). The innovation is an attention-based fusion: ( \\mathbf{z}t\u0026rsquo; = \\sum{s \\neq t} \\alpha{ts} \\mathbf{z}s ), where ( \\alpha{ts} ) weights contributions from other tasks, computed via a task-to-task attention MLP. Outputs are per-task sigmoids: ( \\hat{y}_t = \\sigma(\\mathbf{w}_t^T \\mathbf{z}_t\u0026rsquo;) ).\nThe loss is a weighted sum: ( L = \\sum_t \\lambda_t L_t ) (binary cross-entropy per task), optimized with Adam. On Kuaishou‚Äôs dataset, AdaTT lifted CTR AUC by 1.5% and CVR AUC by 2% over single-task models, thanks to its adaptive fusion‚Äîa leap over static MTL. Its complexity, though, demands careful tuning.\nConclusion: What‚Äôs Next From NCF‚Äôs non-linear leap to AdaTT‚Äôs multi-task finesse, DNNs have reshaped RecSys, each model solving a prior limitation: NCF broke linearity, Wide \u0026amp; Deep merged memorization and generalization, DeepFM automated engineering, DIN added sequence, DLRM scaled up, and AdaTT tackled multiple goals. Part 4 will explore graph neural networks and transformers, pushing RecSys further into complex, real-time domains.\nReferences He, X., et al. (2017). Neural Collaborative Filtering. arXiv:1708.05031. Cheng, H.-T., et al. (2016). Wide \u0026amp; Deep Learning for Recommender Systems. arXiv:1606.07792. Guo, H., et al. (2017). DeepFM: A Factorization-Machine Based Neural Network for CTR Prediction. arXiv:1703.04247. Zhou, G., et al. (2017). Deep Interest Network for Click-Through Rate Prediction. arXiv:1706.06978. Naumov, M., et al. (2019). Deep Learning Recommendation Model for Personalization and Recommendation Systems. arXiv:1906.00091. Yang, S., et al. (2023). AdaTT: Adaptive Task-to-Task Fusion Network for Multitask Learning in Recommendations. arXiv:2304.04959. ","permalink":"//localhost:1313/posts/the-evaluation-of-recsys-part-3/","summary":"\u003ch1 id=\"the-evaluation-of-recommendation-systems---part-3\"\u003eThe Evaluation of Recommendation Systems - Part 3\u003c/h1\u003e\n\u003ch2 id=\"context-why-this-post-matters-who-its-for-and-what-youll-learn\"\u003eContext: Why This Post Matters, Who It‚Äôs For, and What You‚Äôll Learn\u003c/h2\u003e\n\u003cp\u003eWelcome to Part 3 of our four-part series on evaluating recommendation systems (RecSys)! In the previous installments, we laid the groundwork: Part 1 introduced foundational techniques like collaborative filtering (CF) and Matrix Factorization (MF), which excelled at capturing user-item interactions but assumed linearity, missing complex patterns. Part 2 explored Factorization Machines (FM) and XGBoost, which tackled sparse data and non-linear ranking but fell short on higher-order interactions and sequential behaviors. By 2016, these limitations spurred a seismic shift toward deep neural networks (DNNs), which transformed RecSys by learning intricate feature interactions, automating feature engineering, and addressing diverse tasks like sequential recommendations and multi-task optimization. This post traces that evolution from 2016 to 2023, diving into Neural Collaborative Filtering (NCF), Wide \u0026amp; Deep Learning, DeepFM, Deep Interest Network (DIN), Deep Learning Recommendation Model (DLRM), and Adaptive Task-to-Task Fusion (AdaTT). It‚Äôs tailored for data scientists, ML engineers, and tech professionals‚Äîparticularly those designing large-scale RecSys in domains like e-commerce, streaming, and advertising‚Äîwho need a deep, technical understanding of these advancements.\u003c/p\u003e","title":"The Evaluation of RecSys - Part 3"},{"content":"Welcome to Part 2 of RecSys series! In Part 1, we traced the evolution of RecSys from content-based filtering (CBF) to collaborative filtering (CF), and finally to Matrix Factorization (MF), which introduced latent factor models to tackle sparsity and scalability. However, MF‚Äôs linear assumptions and struggles with implicit feedback (e.g., clicks, views) set the stage for more advanced techniques. In this post, we dive into two pivotal methods from the 2010-2015 era: Factorization Machines (FM) and Gradient Boosted Trees (XGBoost).\nIn Part 2, you‚Äôll learn how FM generalizes MF to handle diverse data types, how XGBoost leverages decision trees for ranking, and the strengths and limitations of each.\n1. Recap In Part 1, we explored the foundational stages of RecSys:\nCBF based on features (e.g., movie genres) but struggled with diversity. CF leveraged user-item interactions, introducing neighborhood methods and latent factor models. MF modeled users and items in a latent space, predicting ratings as \\(\\hat{r}_{ui} = p_u^T q_i\\). However, MF assumed linear interactions and worked best with explicit feedback (ratings) only, failed to capture implicit feedback like clicks or views. These limitations prompted the 2010-2015 era, where machine learning techniques like FM and XGBoost emerged to handle more complex patterns.\n2. In Layman terms Imagine you‚Äôre shopping in a store for a jacket. In Part 1, MF was like a salesman who suggested jackets based on your ratings, guessing your taste with simple categories like ‚Äúlikes warm jackets‚Äù or ‚Äúprefers casual style.‚Äù It worked well when you rated items, but what if you never rating? or what if the assistant only knows you clicked on a jacket or viewed its picture? MF struggles here because it‚Äôs too rigid. Enter Factorization Machines (FM) and XGBoost‚Äîtwo smarter assistants who arrived around 2010 to fix this.\nFM: It\u0026rsquo;s like a smart salesman who looks at everything‚Äîyou clicked, the weather, and your profile (e.g., you‚Äôre a runner), mixing these clues to suggest a waterproof running jacket if it‚Äôs rainy. It‚Äôs flexible and can handle all kinds of hints, not just ratings.\nXGBoost: XGBoost is like a super-smart friend who learns from everyone‚Äôs shopping habits to suggest the perfect jacket. It builds a decision flowchart (Actually TREE): ‚ÄúIf you like bright colors, and it‚Äôs winter, and you often buy on weekends, then try this red parka.‚Äù It improves its suggestions step by step.\nThese assistants are more flexible than MF, handling messy data and complex patterns, but they have limits, which we‚Äôll explore as we move toward deep learning in Part 3.\n3. Prerequisites Dot product combines two vectors to measure similarity‚Äîthink of it as a handshake between features (e.g., user preferences and item traits). Loss function measures prediction errors (e.g., squared error: \\((y - \\hat{y})^2\\)), regularization prevents overfitting, and optimization (e.g., gradient descent) minimizes the loss. One-hot encoding transforming raw data (e.g., user IDs, item categories) into usable inputs. From Part 1, recall MF models ratings as \\(\\hat{r}_{ui} = p_u^T q_i\\), where \\(p_u\\) and \\(q_i\\) are latent vectors, but it struggles with implicit feedback. For more on these topics, check out Linear Algebra Basics or Intro to Machine Learning.\n4. Deep Dive 4.1 Factorization Machines (FM) FM, introduced by Steffen Rendle in 2010, generalizes Matrix Factorization to model interactions between any features, not just users and items. It excels in sparse, high, dimensional settings like CTR prediction in online advertising, where data includes implicit feedback (clicks, views) and diverse features (user demographics, ad categories, context). FM‚Äôs ability to capture pairwise feature interactions without manual engineering made it a cornerstone for RecSys.\nHow It Works:\nFM models a prediction (e.g., click probability) as a combination of linear and pairwise feature interactions. For a feature vector \\(\\mathbf{x} \\in \\mathbb{R}^n\\) (where \\(n\\) is the number of features), the prediction is:\n$$ \\hat{y}(\\mathbf{x}) = w_0 + \\sum_{i=1}^n w_i x_i + \\sum_{i=1}^n \\sum_{j=i+1}^n \\langle v_i, v_j \\rangle x_i x_j $$\n\\(w_0\\): Global bias. \\(w_i\\): Weight for feature \\(x_i\\). \\(\\langle v_i, v_j \\rangle = v_i^T v_j\\): Dot product of latent vectors \\(v_i, v_j \\in \\mathbb{R}^k\\), modeling the interaction between features \\(x_i\\) and \\(x_j\\). \\(k\\): Number of latent factors (typically 10-100). This captures both linear effects ( \\(w_i x_i\\)) and pairwise interactions ( \\(\\langle v_i, v_j \\rangle x_i x_j\\)). For example, in CTR prediction, \\(x_i\\) might indicate the user, \\(x_j\\) the ad, and their interaction reflects compatibility.\nConnection to MF:\nIf \\(\\mathbf{x}\\) encodes only user \\(u\\) and item \\(i\\) (e.g., \\(x_u = 1\\), \\(x_i = 1\\), all others 0), FM reduces to MF: $$ \\hat{y}(\\mathbf{x}) = w_0 + w_u + w_i + \\langle v_u, v_i \\rangle $$ Here, \\(\\langle v_u, v_i \\rangle\\) mirrors MF‚Äôs \\(p_u^T q_i\\), but FM‚Äôs generality allows modeling additional features like user age or ad category.\nLoss Function:\nFM supports regression (rating prediction) or classification (click prediction). For regression:\n$$ L = \\sum_{(\\mathbf{x}, y) \\in D} (y - \\hat{y}(\\mathbf{x}))^2 + \\lambda (| \\mathbf{w} |_2^2 + | V |_F^2) $$\nFor classification (CTR):\n$$ L = \\sum_{(\\mathbf{x}, y) \\in D} \\log(1 + \\exp(-y \\hat{y}(\\mathbf{x}))) + \\lambda (| \\mathbf{w} |_2^2 + | V |_F^2) $$\n\\(D\\): Training data. \\(y\\): Target (e.g., 1 for click, -1 for no click). \\(\\lambda\\): Regularization to prevent overfitting. \\(V\\): Matrix of latent vectors \\(v_i\\). Optimization:\nRendle (2010) proposes three methods:\nStochastic Gradient Descent (SGD): Updates parameters incrementally for each sample, ideal for large datasets. Alternating Least Squares (ALS): Optimizes one parameter at a time, better for batch processing. Markov Chain Monte Carlo (MCMC): A Bayesian approach, offering uncertainty estimates but slower.\nSGD is often preferred for scalability, with updates like: $$ w_i \\leftarrow w_i - \\eta \\frac{\\partial L}{\\partial w_i}, \\quad v_i \\leftarrow v_i - \\eta \\frac{\\partial L}{\\partial v_i} $$\n\\(\\eta\\): Learning rate. Input and Output:\nInput: Sparse feature vector \\(\\mathbf{x}\\) (e.g., one-hot encoded user ID, item ID, context). Output: Predicted score (e.g., click probability, rating). Real-World Example:\nAt Meta Ads, FM might model user-ad interactions by combining user demographics (e.g., age, location), ad features (e.g., category, keyword), and context (e.g., device type), predicting the likelihood of a click to optimize ad placement.\nTakeaways:\nCaptures pairwise feature interactions. Scales well in sparse, high-dimensional data. Excels in CTR prediction and implicit feedback tasks. Features: Handles both explicit and implicit feedback, scales to high-dimensional sparse data, captures feature interactions without manual engineering.\nLimitations: Limited to pairwise interactions, misses higher-order patterns (e.g., user-item-context triplets), and assumes linear combinations, which may not capture deep non-linearities.\nLed to: Deep learning models like DeepFM (2017), which combine FM with neural networks to learn higher-order interactions.\n4.2 Gradient Boosted Trees (XGBoost) XGBoost, introduced by Chen \u0026amp; Guestrin in 2016, leverages an ensemble of decision trees for ranking tasks in RecSys, excelling in search (e.g., Bing) and online advertising. It addresses MF and FM‚Äôs limitations in capturing non-linear patterns, using second-order optimization for efficiency and scalability.\nHow It Works:\nXGBoost builds a sequence of decision trees, each correcting the errors of the previous ones. For RecSys, it‚Äôs often used in learning-to-rank tasks (e.g., ranking search results or videos). Features include user behavior (clicks, watch time), item metadata (category, tags), and context (time, device). The prediction is:\n$$ \\hat{y}i = \\sum{t=1}^T f_t(\\mathbf{x}_i) $$\n\\(T\\): Number of trees. \\(f_t\\): Output of the \\(t\\)-th tree for input \\(\\mathbf{x}_i\\). Loss Function:\nXGBoost optimizes a regularized objective:\n$$ L = \\sum_{i=1}^N l(y_i, \\hat{y}i) + \\sum{t=1}^T \\Omega(f_t) $$\n\\(l(y_i, \\hat{y}_i)\\): Loss, e.g., squared loss for regression, or pairwise ranking loss (e.g., LambdaRank). \\(\\Omega(f_t) = \\gamma T + \\frac{1}{2} \\lambda \\| \\mathbf{w}_t \\|_2^2\\): Regularization, where \\(T\\) is the number of leaves, and \\(\\mathbf{w}_t\\) are leaf weights. For ranking, it uses pairwise loss:\n$$ L = \\sum_{(i,j) \\in P} \\log(1 + \\exp(-(\\hat{y}_i - \\hat{y}_j))) $$\nwhere \\(P\\) is the set of relevant-irrelevant pairs. XGBoost uses a second-order approximation:\n$$ L \\approx \\sum_{i=1}^N \\left[ l(y_i, \\hat{y}_i^{(t-1)}) + g_i f_t(\\mathbf{x}_i) + \\frac{1}{2} h_i f_t(\\mathbf{x}_i)^2 \\right] + \\Omega(f_t) $$\n\\(g_i = \\frac{\\partial l}{\\partial \\hat{y}_i^{(t-1)}}\\), \\(h_i = \\frac{\\partial^2 l}{\\partial (\\hat{y}_i^{(t-1)})^2}\\). This enables efficient tree construction, with features like column sampling and parallel processing for scalability.\nInput and Output:\nInput: Feature vectors (numerical or categorical, e.g., user watch time, item category). Output: Ranking scores for items. Real-World Example:\nAt Bing, XGBoost ranks search results by modeling features like query relevance, user click history, and page quality, ensuring the most relevant results appear at the top.\nKey Takeaways (XGBoost in 3 Points):\nCaptures non-linear patterns via tree ensembles. Robust to missing data and interpretable (feature importance). Excels in ranking tasks like search and ads. Features: Captures non-linear patterns, handles mixed feature types via tree splits, robust to missing data, interpretable (feature importance scores).\nLimitations: Struggles with extremely high-dimensional sparse data (e.g., one-hot encoded user/item IDs), computationally expensive for large datasets, requires careful feature engineering.\nLed to: Neural Collaborative Filtering (NCF) and other deep learning methods that automatically learn feature representations.\n6. Conclusion Part 2 has taken us from MF to FM‚Äôs flexible feature interactions and XGBoost‚Äôs non-linear ranking power. FM excels in CTR prediction by modeling sparse, implicit data, while XGBoost dominates ranking tasks with its ability to capture complex patterns. However, both methods hit limits‚ÄîFM‚Äôs pairwise focus and XGBoost‚Äôs reliance on feature engineering couldn‚Äôt keep up with the complexity of modern RecSys. In Part 3, we‚Äôll explore how deep learning overcomes these limitations, tackling unstructured data like images and text with models like Neural Collaborative Filtering and DeepFM, which leverage neural networks for higher-order interactions and automated feature learning.\n7. References Rendle, S. (2010). Factorization Machines. 2010 IEEE International Conference on Data Mining (ICDM), 995-1000. https://www.ismll.uni-hildesheim.de/pub/pdfs/Rendle2010FM.pdf Chen, T., \u0026amp; Guestrin, C. (2016). XGBoost: A Scalable Tree Boosting System. arXiv preprint arXiv:1603.02754. https://arxiv.org/pdf/1603.02754 He, X., Zhang, H., Kan, M.-Y., \u0026amp; Chua, T.-S. (2017). Fast Matrix Factorization for Online Recommendation with Implicit Feedback. Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR ‚Äô17), 549‚Äì558. https://dl.acm.org/doi/10.1145/3459637.3482492 ","permalink":"//localhost:1313/posts/the-evaluation-of-recsys-part2/","summary":"Sample article showcasing basic code syntax and formatting for HTML elements.","title":"The Evaluation of RecSys - Part 2"},{"content":"This article offers a sample of basic Markdown syntax that can be used in Hugo content files, also it shows whether basic HTML elements are decorated with CSS in a Hugo theme.\nHeadings The following HTML \u0026lt;h1\u0026gt;‚Äî\u0026lt;h6\u0026gt; elements represent six levels of section headings. \u0026lt;h1\u0026gt; is the highest section level while \u0026lt;h6\u0026gt; is the lowest.\nH1 H2 H3 H4 H5 H6 Paragraph Xerum, quo qui aut unt expliquam qui dolut labo. Aque venitatiusda cum, voluptionse latur sitiae dolessi aut parist aut dollo enim qui voluptate ma dolestendit peritin re plis aut quas inctum laceat est volestemque commosa as cus endigna tectur, offic to cor sequas etum rerum idem sintibus eiur? Quianimin porecus evelectur, cum que nis nust voloribus ratem aut omnimi, sitatur? Quiatem. Nam, omnis sum am facea corem alique molestrunt et eos evelece arcillit ut aut eos eos nus, sin conecerem erum fuga. Ri oditatquam, ad quibus unda veliamenimin cusam et facea ipsamus es exerum sitate dolores editium rerore eost, temped molorro ratiae volorro te reribus dolorer sperchicium faceata tiustia prat.\nItatur? Quiatae cullecum rem ent aut odis in re eossequodi nonsequ idebis ne sapicia is sinveli squiatum, core et que aut hariosam ex eat.\nBlockquotes The blockquote element represents content that is quoted from another source, optionally with a citation which must be within a footer or cite element, and optionally with in-line changes such as annotations and abbreviations.\nBlockquote without attribution Tiam, ad mint andaepu dandae nostion secatur sequo quae. Note that you can use Markdown syntax within a blockquote.\nBlockquote with attribution Don\u0026rsquo;t communicate by sharing memory, share memory by communicating.\n‚Äî Rob Pike1\nTables Tables aren\u0026rsquo;t part of the core Markdown spec, but Hugo supports them out-of-the-box.\nName Age Bob 27 Alice 23 Inline Markdown within tables Italics Bold Code italics bold code List Types Ordered List First item Second item Third item Unordered List List item Another item And another item Nested Unordered list Fruit Apple Orange Banana Dairy Milk Cheese Nested Ordered list Fruit Apple Orange Banana Dairy Milk Cheese Third item Sub One Sub Two Other Elements ‚Äî abbr, sub, sup, kbd, mark GIF is a bitmap image format.\nH2O\nXn + Yn = Zn\nPress CTRL+ALT+Delete to end the session.\nMost salamanders are nocturnal, and hunt for insects, worms, and other small creatures.\nThe above quote is excerpted from Rob Pike\u0026rsquo;s talk during Gopherfest, November 18, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"//localhost:1313/posts/markdown-syntax/","summary":"\u003cp\u003eThis article offers a sample of basic Markdown syntax that can be used in Hugo content files, also it shows whether basic HTML elements are decorated with CSS in a Hugo theme.\u003c/p\u003e","title":"Markdown Syntax Guide"},{"content":"Inline Code This is Inline Code\nOnly pre This is pre text Code block with backticks \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34; /\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;meta name=\u0026#34;description\u0026#34; content=\u0026#34;Sample article showcasing basic Markdown syntax and formatting for HTML elements.\u0026#34; /\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block with backticks and language specified \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34; /\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;meta name=\u0026#34;description\u0026#34; content=\u0026#34;Sample article showcasing basic Markdown syntax and formatting for HTML elements.\u0026#34; /\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block with backticks and language specified with line numbers 1 2 3 4 5 6 7 8 9 10 11 12 13 14 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34; /\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;meta name=\u0026#34;description\u0026#34; content=\u0026#34;Sample article showcasing basic Markdown syntax and formatting for HTML elements.\u0026#34; /\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block with line numbers and highlighted lines PaperMod supports linenos=true or linenos=table 1 2 3 4 5 6 7 8 9 10 11 12 13 14 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34; /\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;meta name=\u0026#34;description\u0026#34; content=\u0026#34;Sample article showcasing basic Markdown syntax and formatting for HTML elements.\u0026#34; /\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; With linenos=inline line might not get highlighted properly. This issue is fixed with 045c084 1\u0026lt;!DOCTYPE html\u0026gt; 2\u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; 3 \u0026lt;head\u0026gt; 4 \u0026lt;meta charset=\u0026#34;utf-8\u0026#34; /\u0026gt; 5 \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; 6 \u0026lt;meta 7 name=\u0026#34;description\u0026#34; 8 content=\u0026#34;Sample article showcasing basic Markdown syntax and formatting for HTML elements.\u0026#34; 9 /\u0026gt; 10 \u0026lt;/head\u0026gt; 11 \u0026lt;body\u0026gt; 12 \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; 13 \u0026lt;/body\u0026gt; 14\u0026lt;/html\u0026gt; Code block indented with four spaces \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026quot;utf-8\u0026quot;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block with Hugo\u0026rsquo;s internal highlight shortcode \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Github Gist ","permalink":"//localhost:1313/posts/code_syntax/","summary":"Sample article showcasing basic code syntax and formatting for HTML elements.","title":"Code Syntax Guide"},{"content":"Hugo ships with several Built-in Shortcodes for rich content, along with a Privacy Config and a set of Simple Shortcodes that enable static and no-JS versions of various social media embeds.\nFigure Shortcode (PaperMod enhanced) Photo by Aditya Telange on Unsplash\nYouTube Twitter Shortcode PaperMod is now the most starred @GoHugoIO theme on #GitHub ! ‚ú®\nHere\u0026#39;s what it offers:\n- Simple, minimal \u0026amp; clean design\n- Light/Dark mode\n- Fuzzy search for content\n- Good page-speed insights\nand much more...\nHuge thanks to all supportersüôèhttps://t.co/YAEd2cfrrn\n\u0026mdash; Aditya (@adityatelange) November 14, 2023 Vimeo Shortcode ","permalink":"//localhost:1313/posts/rich-content/","summary":"\u003cp\u003eHugo ships with several \u003ca href=\"https://gohugo.io/content-management/shortcodes/#use-hugos-built-in-shortcodes\"\u003eBuilt-in Shortcodes\u003c/a\u003e for rich content, along with a \u003ca href=\"https://gohugo.io/about/hugo-and-gdpr/\"\u003ePrivacy Config\u003c/a\u003e and a set of Simple Shortcodes that enable static and no-JS versions of various social media embeds.\u003c/p\u003e","title":"Rich Content and Shortcodes"},{"content":"Mathematical notation in a Hugo project can be enabled by using third party JavaScript libraries.\nIn this example we will be using KaTeX\nCreate a partial under /layouts/partials/math.html Within this partial reference the Auto-render Extension or host these scripts locally. Include the partial in your templates (extend_head.html) like so: refer ISSUE #236 {{ if or .Params.math .Site.Params.math }} {{ partial \u0026#34;math.html\u0026#34; . }} {{ end }} To enable KaTex globally set the parameter math to true in a project\u0026rsquo;s configuration To enable KaTex on a per page basis include the parameter math: true in content files Note: Use the online reference of Supported TeX Functions\nExamples Inline math: \\(\\varphi = \\dfrac{1+\\sqrt5}{2}= 1.6180339887‚Ä¶\\) Block math:\n$$ \\varphi = 1+\\frac{1} {1+\\frac{1} {1+\\frac{1} {1+\\cdots} } } $$\n","permalink":"//localhost:1313/posts/math-typesetting/","summary":"\u003cp\u003eMathematical notation in a Hugo project can be enabled by using third party JavaScript libraries.\u003c/p\u003e","title":"Math Typesetting"},{"content":"Emoji can be enabled in a Hugo project in a number of ways.\nThe emojify function can be called directly in templates or Inline Shortcodes.\nTo enable emoji globally, set enableEmoji to true in your site\u0026rsquo;s configuration and then you can type emoji shorthand codes directly in content files; e.g.\nüôà :see_no_evil: üôâ :hear_no_evil: üôä :speak_no_evil:\nThe Emoji cheat sheet is a useful reference for emoji shorthand codes.\nN.B. The above steps enable Unicode Standard emoji characters and sequences in Hugo, however the rendering of these glyphs depends on the browser and the platform. To style the emoji you can either use a third party emoji font or a font stack; e.g.\n.emoji { font-family: Apple Color Emoji, Segoe UI Emoji, NotoColorEmoji, Segoe UI Symbol, Android Emoji, EmojiSymbols; } ","permalink":"//localhost:1313/posts/emoji-support/","summary":"\u003cp\u003eEmoji can be enabled in a Hugo project in a number of ways.\u003c/p\u003e","title":"Emoji Support"},{"content":"Short\nCurrently, as an LLM Architect, helping Aisera build a LLM foundational model and fine-tuning state-of-the-art LLM reasoning models to automate enterprise solutions.\nPreviously, as an MLE at Meta, I worked on ads ranking, advertiser automation, and personalized shopping ads, using state-of-the-art deep learning techniques and tools. I am part of the Advantage+ Shopping Campaigns Ranking Team, where I contribute to optimizing the ad delivery and performance for Billions of users and advertisers worldwide. Besides, I got a chance to work on Meta\u0026rsquo;s largest open-source LLM\u0026rsquo;s library Llama as a part of the Gen AI collaboration.\nI have a strong academic background in computer science, with a PhD from Florida International University and a Master\u0026rsquo;s degree from King Fahd University of Petroleum \u0026amp; Minerals. During my PhD, I developed a drug recommendation system and a feature selection framework for high dimensional data, using Deeplearning4j and TensorFlow. I also have multiple certifications in cloud computing and machine learning from reputable institutions. I am passionate about applying my skills and knowledge to solve real-world problems and create a positive social impact.\nLong\n","permalink":"//localhost:1313/about/","summary":"about","title":"Abdullah Al Mamun"}]